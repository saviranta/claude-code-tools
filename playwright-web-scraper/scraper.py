"""
Playwright Web Scraper

Scrapes fully-rendered HTML from dynamic pages with:
  - Retry logic (403 backoff, timeout retry, incomplete-page detection)
  - Cookie consent handling
  - Site profile support (load site-profile.json from site-structure-analyzer)
  - Raw HTML saved to output/{id}.html

Usage:
    from scraper import scrape_sync, scrape_multiple, load_site_profile

    # Single item
    result = scrape_sync("item-123", url_template="https://example.com/items/{id}")

    # With site profile (generated by site-structure-analyzer.py)
    profile = load_site_profile()
    result = scrape_sync("item-123", url_template="https://example.com/items/{id}",
                         site_profile=profile)

    # Multiple items
    import asyncio
    results = asyncio.run(scrape_multiple(["item-1", "item-2"],
                                          url_template="https://example.com/items/{id}"))

Requirements:
    pip install playwright
    playwright install chromium
"""

import asyncio
import json
import re
from dataclasses import dataclass
from pathlib import Path
from typing import Literal, Optional

from playwright.async_api import TimeoutError as PlaywrightTimeout
from playwright.async_api import async_playwright

# Default directory for saved HTML files
OUTPUT_DIR = Path("output")

# Default indicators — override via site profile or constructor arguments
DEFAULT_REMOVED_INDICATORS = [
    r"page not found",
    r"404",
    r"item.*not found",
    r"listing.*removed",
    r"no longer available",
    r"does not exist",
]


# ---------------------------------------------------------------------------
# Result type
# ---------------------------------------------------------------------------

@dataclass
class ScrapeResult:
    """Result of scraping a single item."""
    item_id: str
    status: Literal["success", "removed", "error"]
    html: Optional[str] = None
    error_message: Optional[str] = None
    final_url: Optional[str] = None
    bytes: int = 0

    def __post_init__(self):
        if self.html:
            self.bytes = len(self.html.encode("utf-8"))


# ---------------------------------------------------------------------------
# Site profile
# ---------------------------------------------------------------------------

def load_site_profile(profile_path: Path = Path("site-profile.json")) -> Optional[dict]:
    """
    Load a site profile generated by site-structure-analyzer.py.
    Returns None if the file doesn't exist.
    """
    if profile_path.exists():
        return json.loads(profile_path.read_text(encoding="utf-8"))
    return None


def _get_indicators(
    site_profile: Optional[dict],
    present_indicators: Optional[list[str]],
    removed_indicators: Optional[list[str]],
) -> tuple[list[str], list[str]]:
    """Resolve indicators from arguments or site profile."""
    profile_content = (site_profile or {}).get("content", {})

    present = present_indicators or profile_content.get("present_indicators", [])
    removed = removed_indicators or profile_content.get("removed_indicators", DEFAULT_REMOVED_INDICATORS)

    return present, removed


# ---------------------------------------------------------------------------
# Removal detection
# ---------------------------------------------------------------------------

def _is_removed(
    html: str,
    final_url: str,
    item_id: str,
    present_indicators: list[str],
    removed_indicators: list[str],
) -> bool:
    """
    Determine if an item has been removed or is no longer available.

    Strategy:
    1. Positive indicators found → item exists, not removed
    2. Redirected away from the item URL → likely removed
    3. Removal pattern found in page text → removed
    4. Page too short to be real content → removed
    5. No positive indicators defined and none found → assume removed
    """
    html_lower = html.lower()

    # Positive indicators — if any match, the item is definitely present
    for indicator in present_indicators:
        if indicator.lower() in html_lower:
            return False

    # Redirect away from item URL
    if item_id not in final_url:
        return True

    # Removal patterns
    for pattern in removed_indicators:
        if re.search(pattern, html_lower):
            return True

    # Very short pages are usually error pages
    if len(html) < 5000:
        return True

    # No positive indicators configured — can't confirm presence
    if not present_indicators:
        return False  # Give benefit of the doubt if no indicators defined

    return True


# ---------------------------------------------------------------------------
# Core scraping
# ---------------------------------------------------------------------------

async def scrape_item(
    item_id: str,
    url_template: str,
    present_indicators: Optional[list[str]] = None,
    removed_indicators: Optional[list[str]] = None,
    site_profile: Optional[dict] = None,
    headless: bool = True,
    timeout_ms: int = 30000,
    max_retries: int = 3,
    save: bool = True,
    output_dir: Path = OUTPUT_DIR,
) -> ScrapeResult:
    """
    Scrape a single item page.

    Args:
        item_id:             ID inserted into url_template as {id}
        url_template:        URL pattern, e.g. "https://example.com/items/{id}"
        present_indicators:  Text strings confirming real content is present
        removed_indicators:  Regex patterns indicating item is gone
        site_profile:        Output from site-structure-analyzer (auto-loaded if None)
        headless:            Run browser headlessly (default True)
        timeout_ms:          Page load timeout in milliseconds
        max_retries:         Retries on 403 or timeout
        save:                Save raw HTML to output_dir/{item_id}.html
        output_dir:          Directory for saved HTML files
    """
    if site_profile is None:
        site_profile = load_site_profile()

    present, removed = _get_indicators(site_profile, present_indicators, removed_indicators)
    url = url_template.format(id=item_id)

    # Main selector from profile — used to wait for dynamic content
    main_selector = (site_profile or {}).get("content", {}).get("main_selector")

    for attempt in range(1, max_retries + 1):
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=headless)
            context = await browser.new_context(
                user_agent=(
                    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                    "AppleWebKit/537.36 (KHTML, like Gecko) "
                    "Chrome/131.0.0.0 Safari/537.36"
                ),
                viewport={"width": 1920, "height": 1080},
            )
            page = await context.new_page()

            try:
                response = await page.goto(url, wait_until="networkidle", timeout=timeout_ms)
                http_status = response.status if response else None

                # 404 — definitively removed
                if http_status == 404:
                    await browser.close()
                    return ScrapeResult(
                        item_id=item_id,
                        status="removed",
                        final_url=page.url,
                        error_message="HTTP 404",
                    )

                # 403 — retry with backoff
                if http_status == 403:
                    await browser.close()
                    if attempt < max_retries:
                        delay = attempt * 5
                        print(f"  HTTP 403 (attempt {attempt}/{max_retries}), retrying in {delay}s...")
                        await asyncio.sleep(delay)
                        continue
                    return ScrapeResult(
                        item_id=item_id,
                        status="error",
                        error_message=f"HTTP 403 after {max_retries} attempts",
                    )

                # Dismiss cookie consent
                for label in ["Accept all", "Accept", "Accept cookies", "OK", "Agree"]:
                    try:
                        btn = page.locator(f'button:has-text("{label}")')
                        if await btn.count() > 0:
                            await btn.first.click()
                            await page.wait_for_timeout(500)
                            break
                    except Exception:
                        pass

                # Wait for main content
                wait_selector = main_selector or 'main, article, [role="main"]'
                try:
                    await page.wait_for_selector(wait_selector, timeout=5000)
                except Exception:
                    pass  # Continue anyway — selector may not exist on this page

                # Scroll to load lazy content
                await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                await page.wait_for_timeout(1500)
                await page.evaluate("window.scrollTo(0, 0)")
                await page.wait_for_timeout(500)

                html = await page.content()
                final_url = page.url
                await browser.close()

                # Positive indicator found — confirmed success
                for indicator in present:
                    if indicator.lower() in html.lower():
                        result = ScrapeResult(
                            item_id=item_id, status="success", html=html, final_url=final_url
                        )
                        if save:
                            _save_html(item_id, html, output_dir)
                        return result

                # Page seems incomplete — retry
                if len(html) < 50000 and attempt < max_retries:
                    delay = attempt * 5
                    print(
                        f"  Incomplete page (attempt {attempt}/{max_retries}, "
                        f"{len(html):,} bytes), retrying in {delay}s..."
                    )
                    await asyncio.sleep(delay)
                    continue

                # Check for removal
                if _is_removed(html, final_url, item_id, present, removed):
                    return ScrapeResult(
                        item_id=item_id,
                        status="removed",
                        html=html,
                        final_url=final_url,
                        error_message="Item appears to be removed or unavailable",
                    )

                result = ScrapeResult(
                    item_id=item_id, status="success", html=html, final_url=final_url
                )
                if save:
                    _save_html(item_id, html, output_dir)
                return result

            except PlaywrightTimeout:
                await browser.close()
                if attempt < max_retries:
                    delay = attempt * 5
                    print(f"  Timeout (attempt {attempt}/{max_retries}), retrying in {delay}s...")
                    await asyncio.sleep(delay)
                    continue
                return ScrapeResult(
                    item_id=item_id,
                    status="error",
                    error_message=f"Timeout after {max_retries} attempts",
                )
            except Exception as e:
                await browser.close()
                return ScrapeResult(item_id=item_id, status="error", error_message=str(e))

    return ScrapeResult(item_id=item_id, status="error", error_message="Max retries exceeded")


# ---------------------------------------------------------------------------
# Convenience wrappers
# ---------------------------------------------------------------------------

def scrape_sync(item_id: str, url_template: str, **kwargs) -> ScrapeResult:
    """
    Synchronous wrapper for scrape_item.

    Args:
        item_id:       ID inserted into url_template as {id}
        url_template:  URL pattern, e.g. "https://example.com/items/{id}"
        **kwargs:      Passed through to scrape_item
    """
    return asyncio.run(scrape_item(item_id, url_template, **kwargs))


async def scrape_multiple(
    item_ids: list[str],
    url_template: str,
    delay_ms: int = 2000,
    **kwargs,
) -> dict[str, ScrapeResult]:
    """
    Scrape multiple items with a polite delay between requests.

    Args:
        item_ids:      List of item IDs
        url_template:  URL pattern with {id} placeholder
        delay_ms:      Delay between requests in milliseconds (default 2000)
        **kwargs:      Passed through to scrape_item
    """
    # Load site profile once and share across all scrapes
    profile = kwargs.pop("site_profile", None) or load_site_profile()
    results = {}

    for i, item_id in enumerate(item_ids):
        print(f"Scraping {item_id} ({i + 1}/{len(item_ids)})...")
        result = await scrape_item(item_id, url_template, site_profile=profile, **kwargs)
        results[item_id] = result

        if result.status == "success":
            print(f"  ✓ success  {result.bytes:,} bytes")
        elif result.status == "removed":
            print(f"  ✗ removed  {result.error_message}")
        else:
            print(f"  ! error    {result.error_message}")

        if i < len(item_ids) - 1:
            await asyncio.sleep(delay_ms / 1000)

    return results


# ---------------------------------------------------------------------------
# HTML persistence
# ---------------------------------------------------------------------------

def _save_html(item_id: str, html: str, output_dir: Path = OUTPUT_DIR) -> Path:
    """Save raw HTML to output_dir/{item_id}.html"""
    output_dir.mkdir(parents=True, exist_ok=True)
    path = output_dir / f"{item_id}.html"
    path.write_text(html, encoding="utf-8")
    return path


def html_exists(item_id: str, output_dir: Path = OUTPUT_DIR) -> bool:
    """Check if a saved HTML file exists for an item."""
    return (output_dir / f"{item_id}.html").exists()


def load_saved_html(item_id: str, output_dir: Path = OUTPUT_DIR) -> Optional[str]:
    """Load a previously saved HTML file, or None if not found."""
    path = output_dir / f"{item_id}.html"
    return path.read_text(encoding="utf-8") if path.exists() else None


def list_saved(output_dir: Path = OUTPUT_DIR) -> list[str]:
    """List all item IDs that have saved HTML files."""
    if not output_dir.exists():
        return []
    return [f.stem for f in output_dir.glob("*.html")]
